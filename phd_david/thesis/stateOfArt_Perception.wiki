== Perception of the acoustic field ==

\ignore{
* Sound perception is spatial
** We do not listen just an audio signal
** We also perceive where it comes from and how that signal interacts with the environment
** For the same signal perception will be different regarding those factors
** In summary, we do not listen just sound signals
** Human ability is to understand the surrounding acoustic field
}

Sound perception is spatial.
That means that, when a source emits a sound,
we are not just perceiving the emitted sound,
but also the position where the sound was emitted from.
Furthermore, besides source localization,
we are able to perceive information regarding the surrounding environment,
for instance, how indirect the path to the source is,
or the size and materials of the surrounding space.
In summary,
audition is not just about the ability to listen and interpret sounds signals
but also about spatially interpreting the surrounding acoustic field.

In order to artificially reproduce realistic acoustical environments,
we need to know
which are those perceptions and
which are the means for the brain to obtain such spatial information.
* TODO: So, in the following sections...


=== Spatial percepts ===

\ignore{
TODO: Find a place where to explain that
* Which spatial perceptions do we have?
** Position relative to the head
*** Convenient spherical coordinates
*** Azimuth, elevation and distance
** Source width and height
** Velocity?
** Environmental
*** What kind of environment surround the source and the listener
*** Spaciousness, surfaces,
*** Not entering in those aspects
}

Main spatial perception is ''sound source location''.
It is usually expressed
relative to the listener head in spherical coordinates.
Spherical coordinates splits the position into three quantities:
''azimuth'', ''elevation'' and ''distance''.
Azimuth and elevation sets, together, the ''orientation'' towards the source.
Azimuth is the rotation on the horizontal plane and around the vertical axis
while elevation is the angle that
forms the relative source orientation with the horizontal plane.
Expressing orientations this way fits natural to humans
because they are the same neck rotations we use
to orientate the head towards a source.

Figure: fig:ambisonicsConvention figures/ambisonicsConvention
Definition of the spherical coordinates used in this work,
related to the head reference frame.
Frontal direction is azimuth and elevation zero.
Positive azimuth go left and positive elevation go up.

In this work, unless stated otherwise,
we adhere to a common convention using in Ambisonics literature
for elevation an azimuth, which is illustrated in \autoref{fig:ambisonicsConvention},
and can be described as follows:
Zero elevation and azimuth is in frontal direction,
positive azimuths are to the left and negative to the right,
while positive elevation goes up and negative goes down, being 90º and -90º at the poles.
When mapping spherical coordinates to Cartesian coordinates,
a right handed system will be used, so that
positive X goes front,
positive Y goes left, and,
positive Z goes up.


\ignore{
* ''spatial extent'': perceived width, size or massiveness of the sound source @cite:potard2006_thesis
* Extent is perceived, but shape is harder to perceive @cite:potard2003_studyApparentShapeAndWideness
* Extent simulation decorrelation @cite:potard2004_decorrelationSourceWidth
* ''Tonal volume'' concept related to source size @cite:rich1916_tonalVolume
* Tries to understand cognitive phenomenon and mechanisms @cite:boring1926 @cite:Stevens1934 @cite:PerrotBuell1982
** Diotic stimuli, pure tones
** equal loudnes, increased freq implies bigger perceived size
** larger duration, larger perceived size
* Auditory density: apparent compactness, concentration, or hardness of a sound
** In pure tones, density increases with frequency
* @cite:PerrotBuell1982 does tonal volume tests
** with noise besides pure tones
** dicothial besides dicothomial
}

Besides position, other traits
from the spatial aspects of a given source can be perceived.
Experiments by @cite:potard2003_studyApparentShapeAndWideness
discarded the shape of the source as actual percept,
but other traits like size or extent are clearly perceived @cite:potard2006_thesis.
Even in pure tones fed as diotic stimuli provide the feeling of wideness,
as experimented by @cite:rich1916_tonalVolume, @cite:boring1926, and @cite:Stevens1934.
@cite:PerrotBuell1982 also experimented with the perception of auditory density
defined as ''apparent compactness, concentration, or hardness of a sound''.

''Source orientation'' can be a percept
in cases when the sound source is directional.
Experiments by @cite:Neuhoff2001_sourceOrientation,
demonstrate that we can perceive such a trait,
and it can be more clear
in cases when the source is frontal, close or its orientation is dynamic.



* Orientation when it has a unbalanced directivity pattern @cite:Neuhoff2001_sourceOrientation
** Better perceived when frontal, close and dynamic
* Occlusions @cite:FaragBlauerAlim2003_sourceOcclusion
** Diffraction or Attenuation of the direct sound depending on obstacle size and frequency
** Difraction moves apparent position to the borders of the obstacle.
* TODO: velocity
* TODO: Perception of the environment: spaciousness, materials, enclosed sources
* TODO: von Bekese relates those traits to the information required to survive on the wild


=== Multi-modalilty, contrast and learning ===

* TODO: Introduce this section
** Setting up the basic principles of spatial auditory perception
** Cues are contrasts among proximal stimuli
** Percepts are fuzzy constructs upon several cues
** We learn to interprete such cues in a plastic way multi-modal reinforcement


\ignore{
* Human brain uses different cues to co-infer the source location
** @cite:Blauert reviews them
** Limited effectiveness as isolated
** Sensible to source position, content, environment
** Multi-modal inference.
}

Human brain uses different cues to infer sound source location
and other spatial information.
Most cues, in isolation,
are quite limited to infer the source position by themselves.
Cues are less precise or ambiguous or provide no information,
depending on factors such as source orientation, frequency ranges, duration...
So, the brain uses a multi-modal approach
by combining those many cues
to build the source localization perception
and overcoming their limitations as isolated cues
[@cite:Blauert].

\ignore{
* Spectral transformations of the sound depend on the relative source position.
** Environment acoustics
** Listener body: head, torso, pinna
* In absence of the original sound as reference, we use contrast among differently transformed signals
** Memory, we are better localizing familiar sounds
** Inter-aural, pretty invariant of the source position
** Time varying relative position
** Reverberation, self-correlation
}

Most spatial cues rely on orientation dependant spectral transformations.
While a sound travels from the source to the ear channel,
it experiments spectral transformations which are dependant
of the relative sound source position.
Such transformations are introduced
by the acoustics of the environment
and by interactions with listener's body,
mostly with head, torso and pinna (outer ear).
Detecting a transformation requires a relative measure but,
because there is no access to the unmodified sound,
the distal stimulus,
localization cues rely on contrast among
proximal stimuli that have been modified differently.


That's the case for inter-aural cues
that exploit how the sound reaches each ear in a different way.
It is also the case of dynamic cues which
rely on detecting filter variations along the time due
to changes on the source position relative to the head,
either because the source moves itself or because
the listener head is moving.
The dramatic impact of head movements in localization accuracy
has been widely demonstrated experimentaly, for example in
@cite:Wallach1940Role,minimumAduditoryMovementAngle,PerrettNoble1997,Wightman1999,KatoMasaharu_200309.

Memory can be considered another reference for contrast.
As experimented by @cite:localizationOfFamiliarSounds,
on the lack of head movement,
knowing the sound reduces the confusions on the median plane,
where inter-aural cues are useless.
Recognizing a sound and recalling how it is
provides insight about the spectral transformations in effect.

Also, it has been reported, @cite:begault2001direct,
that some kinds of environment reflexions, 
mostly ceiling and floor reflections,
help localization,
although some others environment reflections
just do the opposite effect by adding confusion and errors.
This aspect is still wide open to experimentation.


* Cues are individualized but can be learned
** Cues depend on biometrics, so they are individualized TODO: reference
** Biometrics change along life because of growth or aging and we adapt, requires plasticity
** Biometrics change because accidents and we adapt after transitions TODO: reference
** Biometrics change because we are using someone else's HRTF in an experiment
*** @cite:LearningBinauralCues Experiment of changing ILD and ITD. TODO: Properly review
*** @cite:LearningNewPinna Experiment of learning new pinna






=== Localization cues ===

\ignore{
* Cues
** ITD
** ILD
** Body reflections
** Environment reflections
** Pinna filtering
** Head movement
** Source movement
** Inter-Sensory feedback
* Modifying the signal differently depending on the incoming orientation
** References
*** Other ear
*** Progressive movement (head or source)
*** Familiar sounds
* Dependant on each individual
** Head and body sizes
** Pinna individual
* Learning individual cues
** Biometric parameters evolve with time
** Inter-sensory reinforcement
** Learning binaural cues @cite:LearningBinauralCues 
** Learning new pinna @cite:LearningNewPinna
* Measuring localization accuracy
** Minimum Audible Angle
** Minimum Audible Movement Angle
}


Human visual perception use binocular discrepancies
to infer depth from bidimensional optical projections.
In a similar way,
binaural discrepancies are used to infer sound source position
in audio perception.
Main disparities are
''inter-aural level differences'' (ILD) and
''inter-aural time differences'' (ITD).
They were early identified by @cite:LordRayleigh1907
and later tested experimentally by @cite:

TODO: What about intermediate frequencies
TODO: Review ITD and ILD freq ranges
Inter-aural level differences appear for high frequencies, 1.7kHz and above,
whenever one of the ears has no direct path to the source.
For lower frequencies, those with larger wavelengths than the head size,
the wave can diffract and reach the shadowed ear still with a similar level.
But higher frequencies can not be diffracted,
so, the sound intensity at the shadowed ear is lower
(see \autoref{fig:ILD}).

Time difference cue is significant for low frequencies, below 700 Hz.
For such frequencies,
delay difference can be inferred from phase difference,
while, for high frequencies, such it is ambiguous
(see \autoref{fig:ITD}).

Figure: fig:ILD figures/todo.pdf
Inter-aural Level Difference in high and low frequencies

Figure: fig:ITD figures/todo.pdf
Inter-aural Time Difference in high and low frequencies


According to Blauert, for some orientations ILD and ITD by themselves
can provide resolutions on the perceived localization of one degree.
But ILD and ITD by themselves do not fully localize the sound.
In fact, a locus of source positions,
known as ''confusion cone'',
have the same angle to the ear-to-ear axis
(see figure \ref{fig:confusionCone})
and thus, have the same ITD and ILD.
Another cue we use to localize sounds is
orientation dependant filtering created by the pinna
(outer ear) and body reflections.

Figure: fig:confusionCone figures/todo.pdf
Sounds on the confusion cone have the same ITD and ILD

Perception of the cues are mostly relative,
either to the perception among each ear or
to the memory we have of a familiar sound.
Source motions and listener head motions provide more references to compare with.
In the case of head motion,
the brain already knows which changes in relative position to expect
so it is a unconscious movement we make to better localize sound sources.
Continuous motion of a source or

All those cues, altogether, result in an orientation dependant filtering which
changes the coloration and phase of the original sound.
When the binaural disparities are not enough to know
which are the cues affecting the sound,
movements of either the source or the head
introducing a kind of audition motion parallax which can be used
by the brain to get more precise estimations.


* @cite:Algazi2001_lowFrequencyElevationCues Torso and head diffraction as cues for elevation in low frequencies, pinna has no effect, simplified parametric model
* @cite:shinn_cunningham_2000_toriconfusion not just cone of confusion, hand reachable sound
* @cite:feddersen57 Frequency dependency of ITD and ILD effectiveness to locate sounds
* @cite:minimumAduditoryMovementAngle Moving source effect on localization, and perceiving movement
* @cite:KatoMasaharu_200309 localization test: azimuth, elevation, cloaked pinna or not, head still, free, up-down and left-right movements
* Weber law: just noticeable difference: proportional to the intensity of stimuli
* @cite:PerrettNoble1997
* @cite:Wightman1999 front-back disambiguation by means of head and source movements
* @cite:Wenzel1988personalHrtf importance of individualized HRTF
* @cite:Wenzel1993 nonindividualized mess with elevation and cone of confusion robust ITD and ILD
* @cite:ZotkinAntropometricHrtf anthropometry to choose HF HRTF and synth LF HRTF
* @cite:Brown_SphereHrtfs Snowball synthesized HRTF




\ignore{
Copied from the military report about the precedent effect.

One additional binaural mechanism that plays an important role in sound source localization is
the “precedence effect” (Wallach, Newman, & Rosenzweig, 1949). The precedence effect, also
known as “the law of the first wavefront” (Gardner, 1968; Blauert, 1999) or “Haas effect” (Haas,
1972), is an inhibitory effect that allows one to localize sounds, based on the signal that reaches
the ear first (the direct signal) and inhibits the effects of reflections and reverberation. It applies
to inter-stimulus delays larger than those predicted from the finite dimensions of the human head
but shorter than ~50 ms. If the interval between two sounds is very small (less than 0.8 ms), the
precedence effect does not operate and the sound image is heard in a spatial position defined by
the ITD. However, if the time difference between two brief sounds exceeds 0.8 ms and is shorter
than 5 ms for single clicks and 30 to 50 ms for complex sounds, both sounds are still heard as a
single sound. The location of this fused sound image is determined largely by the location of the
first sound. This is true even if the lagging sound is as much as 10 dB higher than the first sound
(Wallach, et al., 1949). However, at higher intensities of reflections, the shift in an apparent
position of the sound source attributable to the presence of an interaural time delay can be
compensated by the interaural intensity difference inducing the shift in the opposite direction. If
the time delay exceeds 30 to 50 ms, both sounds are not fused and are heard separately as a direct
sound and an echo (see section 4). The precedence effect operates primarily in the horizontal
plane, but it can also be observed in the median plane (Rakerd & Hartmann, 1992, 1994).

The effect of the delayed sound on the spatial position of the fused event depends on the interval
between the lead and lag. The lagging sound tends to “pull” the perceived sound location away
from that of the lead. It is noteworthy that if the primary sound and the secondary sound differ
greatly in their spectral (timbral) characteristics, the precedence effect may not occur. This
means that the sound reflection from the wall, which is highly dissimilar from the original sound,
may be heard separately from the original sound even if the time delay is less than 30 to 50 ms
(Divenyi & Blauert, 1987). The precedence effect does not completely eliminate the effect of
the delayed sound even if its level is relatively low. It makes the delayed sounds part of a single
fused event and it reduces the effect of directional information carried by the delayed sounds.
However, the changes in the pattern of reflections can still be detected and they can affect the
perceived size of the sound source, its loudness, and its timbre (Blauert, 1999).
}




=== Evaluation strategies ===

{
* Metrics
** ''Localization acuity'' (LA)
** ''Minimum audible angle'' (MAA), @cite:minimumAudibleAngle
*** minimum angle between two sources to be perceived as different locations.
*** Which is the minimum
*** Rough tendencies
**** Finer at front
**** Frequency dependency?
*** Testing Methodologies?
** ''Minimum auditory movement angle''' (MAMA) @cite:minimumAduditoryMovementAngle
* Cue Isolation
** Head movements: constraining versus tracking
** Binaural versus free field stimuli
** Ear deafing
** Pinna 
* Reporting strategy
** Eye tracking
** Pointing device
** Acoustic pointer
** Visual pointer
* Learning effects
}




* Localization Accuracy
** @cite:OldfieldParker1984_acuityNormal 
*** Horizontal vs vertical accuracy depending on the orientation, 
*** better in azimuth than with elevation
*** better behind, fuzzy spots at 120-160 azimuth, and at up behind
** @cite:OldfieldParker1984_acuityNoPinna
*** Same with no pinna
*** More back-front reversals
*** Increased elevation error, still perceived
*** No significant impact over azimuth
** @cite:OldfieldParker1986_acuityMonaural
*** Same with deafed ear
*** Increased azimuth errors, still perceived
*** Front-back reversal good but not as good as normal
*** Elevation good but with errors on upper and median ones (justification: contribution of the other ear pinna)




* References on cue learning
** Ashmead, D.H., Davis, D.,Whalen,T, Odom, R. (1991), Sound localization and sensitivity to interaural time differences in human infants. Child Dev 62: 1211-1226.
** Babkoff, H., Muchnik, Ch., Ben-David, N., Furst, M., Even-Zohar, Sh., Hildesheimer, M. (2002). Mapping lateralization of click trains in younger and older populations. Hear Res 165: 117-127.
** Abel, Sh. M., Giguère, Ch., Consoli, A., Papsin, B.C. (2000).The effect of aging on horizontal plane sound localization. J Acoust Soc Am 108 (2): 743-752.
** Brown, K.D. \& Balkany,Th. J. (2007). Benefits of bilateral cochlear implantation: a review. Curr Opin Otolaryngol Head Neck Surg 15: 315-318.
** Laske, R.D.,Varaguth, D., Dillier, N., Binkert, A., Holzmann, D., Huber, A. M. (2009). Subjective and objective results after bilateral cochlear implantation in adults. Otol Neurotol 30: 313-318.




